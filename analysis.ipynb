{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05906a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312263b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  \n",
       "0  edgar/data/3662/0000950170-98-000413.txt  \n",
       "1  edgar/data/3662/0000950170-98-001001.txt  \n",
       "2  edgar/data/3662/0000950172-98-000783.txt  \n",
       "3  edgar/data/3662/0000950170-98-002145.txt  \n",
       "4  edgar/data/3662/0000950172-98-001203.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cik_list = pd.read_excel(\"/Users/melodianking/Downloads/sentimentalanalysis_assignment/cik_list.xlsx\")\n",
    "max_row, max_col = cik_list.shape\n",
    "print(max_row)\n",
    "\n",
    "pd.set_option('display.max_colwidth',100) # to display full text in column\n",
    "cik_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac01fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    edgar/data/3662/0000950170-98-000413.txt\n",
       "1    edgar/data/3662/0000950170-98-001001.txt\n",
       "2    edgar/data/3662/0000950172-98-000783.txt\n",
       "3    edgar/data/3662/0000950170-98-002145.txt\n",
       "4    edgar/data/3662/0000950172-98-001203.txt\n",
       "Name: SECFNAME, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cik_list.SECFNAME.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681ed9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt\n",
       "1    https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-001001.txt\n",
       "2    https://www.sec.gov/Archives/edgar/data/3662/0000950172-98-000783.txt\n",
       "3    https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-002145.txt\n",
       "4    https://www.sec.gov/Archives/edgar/data/3662/0000950172-98-001203.txt\n",
       "Name: SECFNAME, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding the initial structure of the link in secfname\n",
    "link = 'https://www.sec.gov/Archives/'\n",
    "cik_list.SECFNAME = link+cik_list.SECFNAME\n",
    "cik_list.SECFNAME.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5104d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8448ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/melodianking/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/melodianking/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#making the stopword set from basic english and the given list of stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopset = set(w.upper() for w in stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e8631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/Users/melodianking/Downloads/sentimentalanalysis_assignment/StopWords_Generic.txt\"\n",
    "glob.glob(path)\n",
    "for filename in glob.glob(path):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        text = re.sub(r\"\\s+\\|\\s+[\\w]*\" , \"\", text)        \n",
    "        stopset.update(text.upper().split())\n",
    "        #print(len(stopset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a226750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/melodianking/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "d = cmudict.dict()\n",
    "\n",
    "def syllables(word):\n",
    "    #referred from stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return count\n",
    "\n",
    "def nsyl(word):\n",
    "    try:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
    "    except KeyError:\n",
    "        #if word not found in cmudict\n",
    "        return syllables(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aaa7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub('[\\d%/$]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_digits(text)\n",
    "    return text\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_upper_case(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.upper()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopset:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "# def stem_words(words):\n",
    "#     \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "#     stemmer = LancasterStemmer()\n",
    "#     stems = []\n",
    "#     for word in words:\n",
    "#         stem = stemmer.stem(word)\n",
    "#         stems.append(stem)\n",
    "#     return stems\n",
    "\n",
    "# def lemmatize_verbs(words):\n",
    "#     \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmas = []\n",
    "#     for word in words:\n",
    "#         lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "#         lemmas.append(lemma)\n",
    "#     return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_upper_case(words)\n",
    "    words = remove_punctuation(words)\n",
    "#     words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ffa3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variables = ['positive_score','negative_score','polarity_score','average_sentence_length', 'percentage_of_complex_words',\\\n",
    "                   'fog_index','complex_word_count','word_count','uncertainty_score','constraining_score', 'positive_word_proportion',\\\n",
    "                   'negative_word_proportion', 'uncertainty_word_proportion', 'constraining_word_proportion' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "303ee29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-a5777f2468a2>:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  constraining_words_whole_report = pd.Series(name='constraining_words_whole_report')\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "constraining_words_whole_report = pd.Series(name='constraining_words_whole_report')\n",
    "\n",
    "df_col = [sec.lower() + '_' + var for sec,var in itertools.product(section_name,variables) ]\n",
    "df = pd.DataFrame(columns=df_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6be768ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 42)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8a35c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usefull dictionaries\n",
    "master_dict = pd.read_csv('/Users/melodianking/Downloads/sentimentalanalysis_assignment/LoughranMcDonald_MasterDictionary_2020.csv', index_col= 0)\n",
    "constraining_dict = set(pd.read_excel('/Users/melodianking/Downloads/sentimentalanalysis_assignment/constraining_dictionary.xlsx',index_col = 0).index)\n",
    "uncertainty_dict = set(pd.read_excel('/Users/melodianking/Downloads/sentimentalanalysis_assignment/uncertainty_dictionary.xlsx', index_col = 0).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2c63b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIK                                                                          4962\n",
       "CONAME                                                        AMERICAN EXPRESS CO\n",
       "FYRMO                                                                      201407\n",
       "FDATE                                                         2014-07-30 00:00:00\n",
       "FORM                                                                         10-Q\n",
       "SECFNAME    https://www.sec.gov/Archives/edgar/data/4962/0001193125-14-286961.txt\n",
       "Name: 64, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cik_list.loc[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2a86a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all forms locally \n",
    "for i in range(max_row):\n",
    "    text = requests.get(cik_list.SECFNAME[i]).text\n",
    "    file_name = 'form' + str(i)\n",
    "    f = open(file_name, 'a+')\n",
    "    f.write(text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "898eec48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading.. 0|0 0|1 0|2 reading.. 1|0 1|1 1|2 reading.. 2|0 2|1 2|2 reading.. 3|0 3|1 3|2 reading.. 4|0 4|1 4|2 reading.. 5|0 5|1 5|2 reading.. 6|0 6|1 6|2 reading.. 7|0 7|1 7|2 reading.. 8|0 8|1 8|2 reading.. 9|0 9|1 9|2 reading.. 10|0 10|1 10|2 reading.. 11|0 11|1 11|2 reading.. 12|0 12|1 12|2 reading.. 13|0 13|1 13|2 reading.. 14|0 14|1 14|2 reading.. 15|0 15|1 15|2 reading.. 16|0 16|1 16|2 reading.. 17|0 17|1 17|2 reading.. 18|0 18|1 18|2 reading.. 19|0 19|1 19|2 reading.. 20|0 20|1 20|2 reading.. 21|0 21|1 21|2 reading.. 22|0 22|1 22|2 reading.. 23|0 23|1 23|2 reading.. 24|0 24|1 24|2 reading.. 25|0 25|1 25|2 reading.. 26|0 26|1 26|2 reading.. 27|0 27|1 27|2 reading.. 28|0 28|1 28|2 reading.. 29|0 29|1 29|2 reading.. 30|0 30|1 30|2 reading.. 31|0 31|1 31|2 reading.. 32|0 32|1 32|2 reading.. 33|0 33|1 33|2 reading.. 34|0 34|1 34|2 reading.. 35|0 35|1 35|2 reading.. 36|0 36|1 36|2 reading.. 37|0 37|1 37|2 reading.. 38|0 38|1 38|2 reading.. 39|0 39|1 39|2 reading.. 40|0 40|1 40|2 reading.. 41|0 41|1 41|2 reading.. 42|0 42|1 42|2 reading.. 43|0 43|1 43|2 reading.. 44|0 44|1 44|2 reading.. 45|0 45|1 45|2 reading.. 46|0 46|1 46|2 reading.. 47|0 47|1 47|2 reading.. 48|0 48|1 48|2 reading.. 49|0 49|1 49|2 reading.. 50|0 50|1 50|2 reading.. 51|0 51|1 51|2 reading.. 52|0 52|1 52|2 reading.. 53|0 53|1 53|2 reading.. 54|0 54|1 54|2 reading.. 55|0 55|1 55|2 reading.. 56|0 56|1 56|2 reading.. 57|0 57|1 57|2 reading.. 58|0 58|1 58|2 reading.. 59|0 59|1 59|2 reading.. 60|0 60|1 60|2 reading.. 61|0 61|1 61|2 reading.. 62|0 62|1 62|2 reading.. reading.. reading.. 65|0 65|1 65|2 reading.. 66|0 66|1 66|2 reading.. 67|0 67|1 67|2 reading.. 68|0 68|1 68|2 reading.. 69|0 69|1 69|2 reading.. 70|0 70|1 70|2 reading.. 71|0 71|1 71|2 reading.. 72|0 72|1 72|2 reading.. 73|0 73|1 73|2 reading.. 74|0 74|1 74|2 reading.. 75|0 75|1 75|2 reading.. 76|0 76|1 76|2 reading.. 77|0 77|1 77|2 reading.. 78|0 78|1 78|2 reading.. 79|0 79|1 79|2 reading.. 80|0 80|1 80|2 reading.. 81|0 81|1 81|2 reading.. 82|0 82|1 82|2 reading.. 83|0 83|1 83|2 reading.. 84|0 84|1 84|2 reading.. 85|0 85|1 85|2 reading.. 86|0 86|1 86|2 reading.. 87|0 87|1 87|2 reading.. 88|0 88|1 88|2 reading.. 89|0 89|1 89|2 reading.. 90|0 90|1 90|2 reading.. 91|0 91|1 91|2 reading.. 92|0 92|1 92|2 reading.. 93|0 93|1 93|2 reading.. 94|0 94|1 94|2 reading.. 95|0 95|1 95|2 reading.. 96|0 96|1 96|2 reading.. 97|0 97|1 97|2 reading.. 98|0 98|1 98|2 reading.. 99|0 99|1 99|2 reading.. 100|0 100|1 100|2 reading.. 101|0 101|1 101|2 reading.. 102|0 102|1 102|2 reading.. 103|0 103|1 103|2 reading.. 104|0 104|1 104|2 reading.. 105|0 105|1 105|2 reading.. 106|0 106|1 106|2 reading.. 107|0 107|1 107|2 reading.. 108|0 108|1 108|2 reading.. 109|0 109|1 109|2 reading.. 110|0 110|1 110|2 reading.. 111|0 111|1 111|2 reading.. 112|0 112|1 112|2 reading.. 113|0 113|1 113|2 reading.. 114|0 114|1 114|2 reading.. 115|0 115|1 115|2 reading.. 116|0 116|1 116|2 reading.. 117|0 117|1 117|2 reading.. 118|0 118|1 118|2 reading.. 119|0 119|1 119|2 reading.. 120|0 120|1 120|2 reading.. 121|0 121|1 121|2 reading.. 122|0 122|1 122|2 reading.. 123|0 123|1 123|2 reading.. 124|0 124|1 124|2 reading.. 125|0 125|1 125|2 reading.. 126|0 126|1 126|2 reading.. 127|0 127|1 127|2 reading.. 128|0 128|1 128|2 reading.. 129|0 129|1 129|2 reading.. 130|0 130|1 130|2 reading.. 131|0 131|1 131|2 reading.. 132|0 132|1 132|2 reading.. 133|0 133|1 133|2 reading.. 134|0 134|1 134|2 reading.. 135|0 135|1 135|2 reading.. 136|0 136|1 136|2 reading.. 137|0 137|1 137|2 reading.. 138|0 138|1 138|2 reading.. 139|0 139|1 139|2 reading.. 140|0 140|1 140|2 reading.. 141|0 141|1 141|2 reading.. 142|0 142|1 142|2 reading.. 143|0 143|1 143|2 reading.. 144|0 144|1 144|2 reading.. 145|0 145|1 145|2 reading.. 146|0 146|1 146|2 reading.. 147|0 147|1 147|2 reading.. 148|0 148|1 148|2 reading.. 149|0 149|1 149|2 reading.. 150|0 150|1 150|2 reading.. 151|0 151|1 151|2 "
     ]
    }
   ],
   "source": [
    "for i in range(max_row):\n",
    "    #print(i)\n",
    "    file_name = '/Users/melodianking/Downloads/sentimentalanalysis_assignment/form' + str(i)\n",
    "    text = open(file_name,'r').read()\n",
    "    print('reading..',end = \" \")\n",
    "    \n",
    "    #constraining_words_whole_report\n",
    "    \n",
    "#     constraining_words_whole_report_count = 0\n",
    "#     for word in denoise_text(text).split():\n",
    "#         if word in constraining_dict:\n",
    "#             constraining_words_whole_report_count += 1\n",
    "#     print('here...',end = \"  \")\n",
    "#     constraining_words_whole_report.loc[i] = constraining_words_whole_report_count\n",
    "    \n",
    "    ####################################\n",
    "    df.loc[i] = np.zeros(42)\n",
    "    # other variable per sections\n",
    "    for j in range(3):\n",
    "        if i in [63,64]:\n",
    "            continue\n",
    "        print(i,j,sep= '|',end = \" \")\n",
    "        exp = r\".*(?P<start>ITEM [\\d]\\. \" + re.escape(section[j]) + r\")(?P<MDA>.*)(?P<body>[\\s\\S]*)(?P<end>ITEM \\d|SIGNATURES)\"\n",
    "        regexp = re.compile(exp)\n",
    "        s = regexp.search(text)\n",
    "        \n",
    "        if s:\n",
    "            data = s.group('body')\n",
    "            text = denoise_text(data)\n",
    "            sent_list = sent_tokenize(text)\n",
    "            sentence_length = len(sent_list)\n",
    "\n",
    "            sample = text.split()\n",
    "            sample = normalize(sample)\n",
    "            word_count = len(sample)\n",
    "            complex_word_count = 0\n",
    "            \n",
    "            for word in sample:\n",
    "                if nsyl(word.lower()) > 2:\n",
    "                    complex_word_count += 1\n",
    "            \n",
    "            average_sentence_length = word_count/sentence_length\n",
    "            percentage_of_complex_words = complex_word_count/word_count\n",
    "            fog_index = 0.4 * (average_sentence_length + percentage_of_complex_words)\n",
    "            \n",
    "            positive_score = 0\n",
    "            negative_score = 0\n",
    "            uncertainty_score = 0\n",
    "            constraining_score = 0\n",
    "            for word in sample:\n",
    "                if word in master_dict.index:\n",
    "                    #print(\"is here\")\n",
    "                    if master_dict.loc[word].Positive > 0:\n",
    "                        #print(\"positive\")\n",
    "                        positive_score += 1\n",
    "                    if master_dict.loc[word].Negative > 0:\n",
    "                        negative_score += 1\n",
    "                    if word in uncertainty_dict:\n",
    "                        uncertainty_score += 1\n",
    "                    if word in constraining_dict:\n",
    "                        constraining_score += 1\n",
    "            #print(positive_score)\n",
    "            polarity_score = (positive_score-negative_score)/(positive_score + negative_score + .000001)\n",
    "            positive_word_proportion = positive_score/word_count\n",
    "            negative_word_proportion = negative_score/word_count\n",
    "            uncertainty_word_proportion = uncertainty_score/word_count\n",
    "            constraining_word_proportion = constraining_score/word_count\n",
    "            \n",
    "            df.loc[i][section_name[j].lower() + \"_positive_score\"] = positive_score\n",
    "            df.loc[i][section_name[j].lower() + \"_negative_score\"] = negative_score\n",
    "            df.loc[i][section_name[j].lower() + \"_polarity_score\"] = polarity_score\n",
    "            df.loc[i][section_name[j].lower() + \"_average_sentence_length\"] = average_sentence_length\n",
    "            df.loc[i][section_name[j].lower() + \"_percentage_of_complex_words\"] = percentage_of_complex_words\n",
    "            df.loc[i][section_name[j].lower() + \"_fog_index\"] = fog_index\n",
    "            df.loc[i][section_name[j].lower() + \"_complex_word_count\"] = complex_word_count\n",
    "            df.loc[i][section_name[j].lower() + \"_word_count\"] = word_count\n",
    "            df.loc[i][section_name[j].lower() + \"_uncertainty_score\"] = uncertainty_score\n",
    "            df.loc[i][section_name[j].lower() + \"_constraining_score\"] = constraining_score\n",
    "            df.loc[i][section_name[j].lower() + \"_positive_word_proportion\"] = positive_word_proportion\n",
    "            df.loc[i][section_name[j].lower() + \"_negative_word_proportion\"] = negative_word_proportion\n",
    "            df.loc[i][section_name[j].lower() + \"_uncertainty_word_proportion\"] = uncertainty_word_proportion\n",
    "            df.loc[i][section_name[j].lower() + \"_constraining_word_proportion\"] = constraining_word_proportion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3dd1097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 reading.. here...  1 reading.. here...  2 reading.. here...  3 reading.. here...  4 reading.. here...  5 reading.. here...  6 reading.. here...  7 reading.. here...  8 reading.. here...  9 reading.. here...  10 reading.. here...  11 reading.. here...  12 reading.. here...  13 reading.. here...  14 reading.. here...  15 reading.. here...  16 reading.. here...  17 reading.. here...  18 reading.. here...  19 reading.. here...  20 reading.. here...  21 reading.. here...  22 reading.. here...  23 reading.. here...  24 reading.. here...  25 reading.. here...  26 reading.. here...  27 reading.. here...  28 reading.. here...  29 reading.. here...  30 reading.. here...  31 reading.. here...  32 reading.. here...  33 reading.. here...  34 reading.. here...  35 reading.. here...  36 reading.. here...  37 reading.. here...  38 reading.. here...  39 reading.. here...  40 reading.. here...  41 reading.. here...  42 reading.. here...  43 reading.. here...  44 reading.. here...  45 reading.. here...  46 reading.. here...  47 reading.. here...  48 reading.. here...  49 reading.. here...  50 reading.. here...  51 reading.. here...  52 reading.. here...  53 reading.. here...  54 reading.. here...  55 reading.. here...  56 reading.. here...  57 reading.. here...  58 reading.. here...  59 reading.. here...  60 reading.. here...  61 reading.. here...  62 reading.. here...  63 reading.. here...  64 reading.. here...  65 reading.. here...  66 reading.. here...  67 reading.. here...  68 reading.. here...  69 reading.. here...  70 reading.. here...  71 reading.. here...  72 reading.. here...  73 reading.. here...  74 reading.. here...  75 reading.. here...  76 reading.. here...  77 reading.. here...  78 reading.. here...  79 reading.. here...  80 reading.. here...  81 reading.. here...  82 reading.. here...  83 reading.. here...  84 reading.. here...  85 reading.. here...  86 reading.. here...  87 reading.. here...  88 reading.. here...  89 reading.. here...  90 reading.. here...  91 reading.. here...  92 reading.. here...  93 reading.. here...  94 reading.. here...  95 reading.. here...  96 reading.. here...  97 reading.. here...  98 reading.. here...  99 reading.. here...  100 reading.. here...  101 reading.. here...  102 reading.. here...  103 reading.. here...  104 reading.. here...  105 reading.. here...  106 reading.. here...  107 reading.. here...  108 reading.. here...  109 reading.. here...  110 reading.. here...  111 reading.. here...  112 reading.. here...  113 reading.. here...  114 reading.. here...  115 reading.. here...  116 reading.. here...  117 reading.. here...  118 reading.. here...  119 reading.. here...  120 reading.. here...  121 reading.. here...  122 reading.. here...  123 reading.. here...  124 reading.. here...  125 reading.. here...  126 reading.. here...  127 reading.. here...  128 reading.. here...  129 reading.. here...  130 reading.. here...  131 reading.. here...  132 reading.. here...  133 reading.. here...  134 reading.. here...  135 reading.. here...  136 reading.. here...  137 reading.. here...  138 reading.. here...  139 reading.. here...  140 reading.. here...  141 reading.. here...  142 reading.. here...  143 reading.. here...  144 reading.. here...  145 reading.. here...  146 reading.. here...  147 reading.. here...  148 reading.. here...  149 reading.. here...  150 reading.. here...  151 reading.. here...  "
     ]
    }
   ],
   "source": [
    "for i in range(max_row):\n",
    "    print(i,end = \" \")\n",
    "    file_name = '/Users/melodianking/Downloads/sentimentalanalysis_assignment/form' + str(i)\n",
    "    text = open(file_name,'r').read()\n",
    "    print('reading..',end = \" \")\n",
    "    \n",
    "    #constraining_words_whole_report\n",
    "    constraining_words_whole_report.loc[i] = 0\n",
    "    constraining_words_whole_report_count = 0\n",
    "    for word in denoise_text(text).split():\n",
    "        if word in constraining_dict:\n",
    "            constraining_words_whole_report_count += 1\n",
    "    print('here...',end = \"  \")\n",
    "    constraining_words_whole_report.loc[i] = constraining_words_whole_report_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00f5904d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 56)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([cik_list,df,constraining_words_whole_report], axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da4dca69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>...</th>\n",
       "      <th>rf_complex_word_count</th>\n",
       "      <th>rf_word_count</th>\n",
       "      <th>rf_uncertainty_score</th>\n",
       "      <th>rf_constraining_score</th>\n",
       "      <th>rf_positive_word_proportion</th>\n",
       "      <th>rf_negative_word_proportion</th>\n",
       "      <th>rf_uncertainty_word_proportion</th>\n",
       "      <th>rf_constraining_word_proportion</th>\n",
       "      <th>constraining_words_whole_report</th>\n",
       "      <th>constraining_words_whole_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-25</td>\n",
       "      <td>10-Q/A</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-002278.txt</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199812</td>\n",
       "      <td>1998-12-22</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-002401.txt</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199812</td>\n",
       "      <td>1998-12-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199812</td>\n",
       "      <td>1998-12-22</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-002402.txt</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199812</td>\n",
       "      <td>1998-12-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199903</td>\n",
       "      <td>1999-03-31</td>\n",
       "      <td>NT 10-K</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0000950172-99-000362.txt</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199903</td>\n",
       "      <td>1999-03-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199905</td>\n",
       "      <td>1999-05-11</td>\n",
       "      <td>10-K</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0000950170-99-000775.txt</td>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199905</td>\n",
       "      <td>1999-05-11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "5  3662  SUNBEAM CORP/FL/  199811 1998-11-25   10-Q/A   \n",
       "6  3662  SUNBEAM CORP/FL/  199812 1998-12-22     10-Q   \n",
       "7  3662  SUNBEAM CORP/FL/  199812 1998-12-22     10-Q   \n",
       "8  3662  SUNBEAM CORP/FL/  199903 1999-03-31  NT 10-K   \n",
       "9  3662  SUNBEAM CORP/FL/  199905 1999-05-11     10-K   \n",
       "\n",
       "                                                                SECFNAME  \\\n",
       "0  https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt   \n",
       "1  https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-001001.txt   \n",
       "2  https://www.sec.gov/Archives/edgar/data/3662/0000950172-98-000783.txt   \n",
       "3  https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-002145.txt   \n",
       "4  https://www.sec.gov/Archives/edgar/data/3662/0000950172-98-001203.txt   \n",
       "5  https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-002278.txt   \n",
       "6  https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-002401.txt   \n",
       "7  https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-002402.txt   \n",
       "8  https://www.sec.gov/Archives/edgar/data/3662/0000950172-99-000362.txt   \n",
       "9  https://www.sec.gov/Archives/edgar/data/3662/0000950170-99-000775.txt   \n",
       "\n",
       "    CIK            CONAME   FYRMO      FDATE  ... rf_complex_word_count  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  ...                   0.0   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15  ...                   0.0   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  ...                   0.0   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12  ...                   0.0   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  ...                   0.0   \n",
       "5  3662  SUNBEAM CORP/FL/  199811 1998-11-25  ...                   0.0   \n",
       "6  3662  SUNBEAM CORP/FL/  199812 1998-12-22  ...                   0.0   \n",
       "7  3662  SUNBEAM CORP/FL/  199812 1998-12-22  ...                   0.0   \n",
       "8  3662  SUNBEAM CORP/FL/  199903 1999-03-31  ...                   0.0   \n",
       "9  3662  SUNBEAM CORP/FL/  199905 1999-05-11  ...                   0.0   \n",
       "\n",
       "  rf_word_count  rf_uncertainty_score  rf_constraining_score  \\\n",
       "0           0.0                   0.0                    0.0   \n",
       "1           0.0                   0.0                    0.0   \n",
       "2           0.0                   0.0                    0.0   \n",
       "3           0.0                   0.0                    0.0   \n",
       "4           0.0                   0.0                    0.0   \n",
       "5           0.0                   0.0                    0.0   \n",
       "6           0.0                   0.0                    0.0   \n",
       "7           0.0                   0.0                    0.0   \n",
       "8           0.0                   0.0                    0.0   \n",
       "9           0.0                   0.0                    0.0   \n",
       "\n",
       "   rf_positive_word_proportion  rf_negative_word_proportion  \\\n",
       "0                          0.0                          0.0   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          0.0   \n",
       "3                          0.0                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "5                          0.0                          0.0   \n",
       "6                          0.0                          0.0   \n",
       "7                          0.0                          0.0   \n",
       "8                          0.0                          0.0   \n",
       "9                          0.0                          0.0   \n",
       "\n",
       "   rf_uncertainty_word_proportion  rf_constraining_word_proportion  \\\n",
       "0                             0.0                              0.0   \n",
       "1                             0.0                              0.0   \n",
       "2                             0.0                              0.0   \n",
       "3                             0.0                              0.0   \n",
       "4                             0.0                              0.0   \n",
       "5                             0.0                              0.0   \n",
       "6                             0.0                              0.0   \n",
       "7                             0.0                              0.0   \n",
       "8                             0.0                              0.0   \n",
       "9                             0.0                              0.0   \n",
       "\n",
       "   constraining_words_whole_report  constraining_words_whole_report  \n",
       "0                               55                               55  \n",
       "1                                0                                0  \n",
       "2                                0                                0  \n",
       "3                                0                                0  \n",
       "4                                0                                0  \n",
       "5                                1                                1  \n",
       "6                               16                               16  \n",
       "7                                0                                0  \n",
       "8                                0                                0  \n",
       "9                               15                               15  \n",
       "\n",
       "[10 rows x 56 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57d20aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "writer = pd.ExcelWriter('/Users/melodianking/Downloads/sentimentalanalysis_assignment/output.xlsx')\n",
    "df.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa2ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
